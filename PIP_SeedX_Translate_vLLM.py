import re
import os
from typing import Optional, List, Tuple

# å°è¯•å¯¼å…¥vLLMï¼ˆå®˜æ–¹æ¨èï¼‰
try:
    from vllm import LLM, SamplingParams
    # æ³¨æ„ï¼šBeamSearchParamsåœ¨vLLMä¸­ä¸å­˜åœ¨ï¼Œä½¿ç”¨SamplingParamsçš„beam searchåŠŸèƒ½
    VLLM_AVAILABLE = True
    print("vLLM is available for this node")
except ImportError as e:
    print(f"vLLM not available: {e}")
    VLLM_AVAILABLE = False
    LLM = None
    SamplingParams = None

# å¯¼å…¥transformersä½œä¸ºå¤‡ç”¨
try:
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    print("Transformers not available")
    TRANSFORMERS_AVAILABLE = False

class PIP_SeedX_Translate_vLLM:
    """
    Seed-X ç¿»è¯‘èŠ‚ç‚¹ - åŸºäºvLLMçš„å®˜æ–¹å®ç°
    å®Œå…¨æŒ‰ç…§å®˜æ–¹READMEå®ç°ï¼Œæ”¯æŒè‡ªåŠ¨è¯­è¨€æ£€æµ‹å’Œæ™ºèƒ½æ··åˆè¯­è¨€å¤„ç†
    """
    
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.use_vllm = VLLM_AVAILABLE
        self.current_model_path = None
        
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "text": ("STRING", {
                    "multiline": True,
                    "default": "Hello, how are you today?"
                }),
                "target_language": (["ä¸­æ–‡", "English", "æ—¥æœ¬èª", "í•œêµ­ì–´", "FranÃ§ais", "Deutsch", "EspaÃ±ol", "Italiano", "Ğ ÑƒÑÑĞºĞ¸Ğ¹"], {
                    "default": "ä¸­æ–‡"
                }),
                "max_length": ("INT", {
                    "default": 512,
                    "min": 50,
                    "max": 2048
                }),
                "temperature": ("FLOAT", {
                    "default": 0.7,
                    "min": 0.0,
                    "max": 2.0,
                    "step": 0.1
                }),
                "do_sample": ("BOOLEAN", {
                    "default": False
                }),
                "num_beams": ("INT", {
                    "default": 4,
                    "min": 1,
                    "max": 8
                }),
                "enable_cot": ("BOOLEAN", {
                    "default": False
                })
            },
            "optional": {
                "model_path": ("STRING", {
                    "default": "models/Seed-X-Instruct-7B"
                })
            }
        }
    
    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("translated_text",)
    FUNCTION = "translate_text"
    CATEGORY = "PIP_Tool"
    
    def _get_language_code(self, language: str) -> str:
        """è·å–è¯­è¨€ä»£ç """
        language_map = {
            "ä¸­æ–‡": "zh",
            "English": "en", 
            "æ—¥æœ¬èª": "ja",
            "í•œêµ­ì–´": "ko",
            "FranÃ§ais": "fr",
            "Deutsch": "de",
            "EspaÃ±ol": "es",
            "Italiano": "it",
            "Ğ ÑƒÑÑĞºĞ¸Ğ¹": "ru"
        }
        return language_map.get(language, "zh")
    
    def _detect_language(self, text: str) -> str:
        """ç®€å•çš„è¯­è¨€æ£€æµ‹"""
        # æ£€æµ‹ä¸­æ–‡å­—ç¬¦
        if re.search(r'[\u4e00-\u9fff]', text):
            return "zh"
        # æ£€æµ‹æ—¥æ–‡å­—ç¬¦
        elif re.search(r'[\u3040-\u309f\u30a0-\u30ff]', text):
            return "ja"
        # æ£€æµ‹éŸ©æ–‡å­—ç¬¦
        elif re.search(r'[\uac00-\ud7af]', text):
            return "ko"
        # æ£€æµ‹ä¿„æ–‡å­—ç¬¦
        elif re.search(r'[\u0400-\u04ff]', text):
            return "ru"
        # é»˜è®¤ä¸ºè‹±æ–‡
        else:
            return "en"
    
    def _get_language_name(self, code: str) -> str:
        """æ ¹æ®ä»£ç è·å–è¯­è¨€åç§°"""
        code_to_name = {
            "zh": "Chinese",
            "en": "English",
            "ja": "Japanese", 
            "ko": "Korean",
            "fr": "French",
            "de": "German",
            "es": "Spanish",
            "it": "Italian",
            "ru": "Russian"
        }
        return code_to_name.get(code, "English")
    
    def _split_mixed_language_text(self, text: str) -> List[Tuple[str, str]]:
        """
        æ™ºèƒ½åˆ†å‰²æ··åˆè¯­è¨€æ–‡æœ¬
        è¿”å›: [(æ–‡æœ¬ç‰‡æ®µ, è¯­è¨€ä»£ç ), ...]
        """
        segments = []
        current_segment = ""
        current_lang = None
        
        for char in text:
            char_lang = self._detect_language(char)
            
            if current_lang is None:
                current_lang = char_lang
                current_segment = char
            elif char_lang == current_lang or char_lang == "en":
                current_segment += char
            else:
                if current_segment.strip():
                    segments.append((current_segment.strip(), current_lang))
                current_segment = char
                current_lang = char_lang
        
        if current_segment.strip():
            segments.append((current_segment.strip(), current_lang))
        
        return segments
    
    def _resolve_model_path(self, model_path: str) -> str:
        """è§£ææ¨¡å‹è·¯å¾„ - ä¼˜å…ˆä½¿ç”¨è„šæœ¬ç›¸å¯¹è·¯å¾„"""
        # å¦‚æœæ˜¯ç»å¯¹è·¯å¾„ï¼Œç›´æ¥è¿”å›
        if os.path.isabs(model_path):
            return model_path
            
        # ç›´æ¥ä½¿ç”¨è„šæœ¬ç›¸å¯¹è·¯å¾„
        script_relative_path = os.path.join(os.path.dirname(__file__), "models", "Seed-X-Instruct-7B")
        if os.path.exists(script_relative_path):
            print(f"Using script relative path: {script_relative_path}")
            return script_relative_path
            
        # å¤‡ç”¨æ–¹æ¡ˆï¼šå°è¯•å…¶ä»–è·¯å¾„
        possible_paths = [
            os.path.join(os.path.dirname(__file__), model_path),  # ç›¸å¯¹äºå½“å‰æ–‡ä»¶
            os.path.join(os.getcwd(), model_path),  # å½“å‰å·¥ä½œç›®å½•
            model_path,  # åŸå§‹è·¯å¾„
        ]
        
        for path in possible_paths:
            abs_path = os.path.abspath(path)
            if os.path.exists(abs_path):
                print(f"Found model at: {abs_path}")
                return abs_path
                
        print(f"Warning: Model not found in any expected location")
        return model_path  # å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œè¿”å›åŸå§‹è·¯å¾„
    
    def _load_model(self, model_path: str) -> bool:
        """åŠ è½½æ¨¡å‹ - ä¼˜å…ˆä½¿ç”¨vLLMï¼Œtransformersä½œä¸ºå¤‡ç”¨"""
        if self.current_model_path == model_path and (self.model or self.tokenizer):
            return True
            
        try:
            resolved_path = self._resolve_model_path(model_path)
            print(f"Loading Seed-X model from: {resolved_path}")
            
            if VLLM_AVAILABLE:
                try:
                    # ä¼˜å…ˆä½¿ç”¨vLLM
                    print("Loading model with vLLM (official method)...")
                    self.model = LLM(
                        model=resolved_path,
                        max_num_seqs=512,
                        enable_prefix_caching=True,
                        gpu_memory_utilization=0.9
                    )
                    self.use_vllm = True
                    self.current_model_path = model_path
                    print("Seed-X model loaded successfully with vLLM!")
                    return True
                except Exception as e:
                    print(f"vLLM loading failed: {str(e)}")
                    self.model = None
            
            if TRANSFORMERS_AVAILABLE:
                try:
                    # æ£€æŸ¥å¿…éœ€çš„tokenizeræ–‡ä»¶
                    import os
                    tokenizer_json_path = os.path.join(resolved_path, "tokenizer.json")
                    if os.path.exists(tokenizer_json_path):
                        with open(tokenizer_json_path, 'r', encoding='utf-8') as f:
                            content = f.read().strip()
                            if not content:
                                print(f"Error: tokenizer.json is empty at {tokenizer_json_path}")
                                print("Please download the complete model from Hugging Face")
                                raise ValueError("Empty tokenizer.json file")
                    
                    # ä½¿ç”¨transformersä½œä¸ºå¤‡ç”¨
                    print("Loading model with transformers (fallback)...")
                    self.tokenizer = AutoTokenizer.from_pretrained(resolved_path)
                    
                    # è®¾ç½®padding token
                    if self.tokenizer.pad_token is None:
                        self.tokenizer.pad_token = self.tokenizer.eos_token
                        print("Set pad_token to eos_token")
                    
                    self.model = AutoModelForCausalLM.from_pretrained(
                        resolved_path,
                        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                        device_map="auto" if torch.cuda.is_available() else None
                    )
                    self.use_vllm = False
                    self.current_model_path = model_path
                    print("Seed-X model loaded successfully with transformers!")
                    return True
                except Exception as e:
                    print(f"Transformers loading failed: {str(e)}")
                    if "tokenizer.json" in str(e) or "Expecting value" in str(e):
                        print("\nğŸš¨ Model files appear to be incomplete or corrupted!")
                        print("Please download the complete Seed-X model from:")
                        print("https://huggingface.co/Seed-X/Seed-X-Instruct-7B")
                        print("\nRequired files:")
                        print("- config.json")
                        print("- tokenizer.json (must not be empty)")
                        print("- tokenizer_config.json")
                        print("- special_tokens_map.json")
                        print("- model.safetensors")
                        print("- generation_config.json")
                    self.model = None
                    self.tokenizer = None
            
            print("Error: No compatible library available for model loading")
            return False
            
        except Exception as e:
            print(f"Error loading model: {str(e)}")
            self.model = None
            self.tokenizer = None
            return False
    
    def _create_translation_prompt(self, text: str, source_lang: str, target_lang: str, enable_cot: bool = False) -> str:
        """åˆ›å»ºç¿»è¯‘æç¤ºè¯ - æŒ‰ç…§å®˜æ–¹æ ¼å¼"""
        source_name = self._get_language_name(source_lang)
        target_name = self._get_language_name(target_lang)
        
        if enable_cot:
            # CoTæ ¼å¼
            prompt = f"Translate the following {source_name} text into {target_name} and explain it in detail:\n{text} <{target_lang}>"
        else:
            # æ ‡å‡†æ ¼å¼
            prompt = f"Translate the following {source_name} text into {target_name}:\n{text} <{target_lang}>"
        
        return prompt
    
    def _generate_translation(self, prompt: str, max_length: int, temperature: float, do_sample: bool, num_beams: int = 4) -> str:
        """ç”Ÿæˆç¿»è¯‘ç»“æœ - ä¼˜å…ˆä½¿ç”¨vLLMï¼Œtransformersä½œä¸ºå¤‡ç”¨"""
        try:
            if self.use_vllm and VLLM_AVAILABLE and hasattr(self, 'model') and self.model:
                # ä½¿ç”¨vLLMç”Ÿæˆ
                if do_sample:
                    # Samplingæ¨¡å¼
                    decoding_params = SamplingParams(
                        temperature=temperature,
                        max_tokens=max_length,
                        skip_special_tokens=True
                    )
                else:
                    # Beam Searchæ¨¡å¼ï¼ˆä½¿ç”¨SamplingParamsçš„beam searchåŠŸèƒ½ï¼‰
                    decoding_params = SamplingParams(
                        temperature=0.0,
                        max_tokens=max_length,
                        best_of=num_beams,
                        use_beam_search=True,
                        skip_special_tokens=True
                    )
                
                # ç”Ÿæˆç¿»è¯‘
                results = self.model.generate([prompt], decoding_params)
                responses = [res.outputs[0].text.strip() for res in results]
                
                return responses[0] if responses and responses[0] else "No translation generated"
            
            elif not self.use_vllm and TRANSFORMERS_AVAILABLE and hasattr(self, 'tokenizer') and self.tokenizer:
                # ä½¿ç”¨transformersä½œä¸ºå¤‡ç”¨
                inputs = self.tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)
                
                if torch.cuda.is_available():
                    inputs = {k: v.cuda() for k, v in inputs.items()}
                
                with torch.no_grad():
                    if do_sample:
                        # Samplingæ¨¡å¼
                        outputs = self.model.generate(
                            **inputs,
                            max_new_tokens=max_length,
                            temperature=temperature,
                            do_sample=True,
                            pad_token_id=self.tokenizer.eos_token_id
                        )
                    else:
                        # Beam Searchæ¨¡å¼
                        outputs = self.model.generate(
                            **inputs,
                            max_new_tokens=max_length,
                            num_beams=num_beams,
                            temperature=temperature,
                            do_sample=False,
                            pad_token_id=self.tokenizer.eos_token_id
                        )
                
                # è§£ç ç»“æœ
                input_length = inputs['input_ids'].shape[1]
                generated_tokens = outputs[0][input_length:]
                generated_text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
                
                return generated_text.strip() if generated_text else "No translation generated"
            
            else:
                return "Error: No compatible model loaded"
            
        except Exception as e:
            print(f"Generation error: {str(e)}")
            return f"Translation error: {str(e)}"
    
    def translate_text(self, text: str, target_language: str, max_length: int = 512, 
                      temperature: float = 0.7, do_sample: bool = False, num_beams: int = 4,
                      enable_cot: bool = False, model_path: str = "models/Seed-X-Instruct-7B") -> tuple:
        """ä¸»è¦ç¿»è¯‘å‡½æ•°"""
        print("got prompt")
        
        # åŠ è½½æ¨¡å‹
        if self.model is None and self.tokenizer is None:
            if not self._load_model(model_path):
                return ("Failed to load model",)
        
        target_code = self._get_language_code(target_language)
        
        # æ£€æµ‹æ··åˆè¯­è¨€å¹¶åˆ†æ®µå¤„ç†
        segments = self._split_mixed_language_text(text)
        translated_segments = []
        
        for segment_text, detected_lang in segments:
            if detected_lang == target_code:
                # å¦‚æœå·²ç»æ˜¯ç›®æ ‡è¯­è¨€ï¼Œç›´æ¥æ·»åŠ 
                translated_segments.append(segment_text)
            else:
                # åˆ›å»ºæç¤ºè¯
                prompt = self._create_translation_prompt(segment_text, detected_lang, target_code, enable_cot)
                
                # ç”Ÿæˆç¿»è¯‘
                translation = self._generate_translation(prompt, max_length, temperature, do_sample, num_beams)
                translated_segments.append(translation)
        
        # åˆå¹¶ç»“æœ
        final_translation = " ".join(translated_segments)
        return (final_translation,)

# ComfyUIèŠ‚ç‚¹æ˜ å°„
NODE_CLASS_MAPPINGS = {
    "PIP_SeedX_Translate_vLLM": PIP_SeedX_Translate_vLLM
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "PIP_SeedX_Translate_vLLM": "Seed-X Translate (vLLM)"
}
